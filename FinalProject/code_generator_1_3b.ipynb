{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOqRMIpvgR19RVwvYLWiasg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMRNblivs1yv","outputId":"d8903de6-6b76-4138-af6e-3fb40cf7cef7","executionInfo":{"status":"ok","timestamp":1747835262262,"user_tz":-540,"elapsed":2457,"user":{"displayName":"김락훈","userId":"17655752783041505657"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKWj24xGs3Ak","outputId":"597a1964-90b6-40fe-d779-71cc90a1ea0b","executionInfo":{"status":"ok","timestamp":1747835262273,"user_tz":-540,"elapsed":3,"user":{"displayName":"김락훈","userId":"17655752783041505657"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Github/AIPlus99/HW6\n"]}],"source":["cd /content/drive/MyDrive/Colab Notebooks/Github/AIPlus99/HW6"]},{"cell_type":"code","source":["!pip install bitsandbytes\n","!pip install flash-attn --no-build-isolation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wLVHCRstR1zq","outputId":"a66965ab-75c5-4a44-9b07-14eb8ea77448","executionInfo":{"status":"ok","timestamp":1747835273912,"user_tz":-540,"elapsed":11639,"user":{"displayName":"김락훈","userId":"17655752783041505657"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n","Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n","Requirement already satisfied: flash-attn in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import re\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n","from torch.cuda.amp import autocast\n","from value_head import ValueHead\n","from transformers import LogitsProcessorList, RepetitionPenaltyLogitsProcessor, TopPLogitsWarper\n","import torch.nn.functional as F\n","import gc\n","\n","def get_quant_config(quantization: str | None):\n","    if quantization == \"4bit\":\n","        return BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=True,\n","            bnb_4bit_quant_type=\"nf4\"\n","        )\n","    elif quantization == \"8bit\":\n","        return BitsAndBytesConfig(\n","            load_in_8bit=True\n","        )\n","    else:\n","        return None\n","\n","def cast_layernorm_and_embedding_to_half(model):\n","    for name, module in model.named_modules():\n","        if isinstance(module, (torch.nn.LayerNorm, torch.nn.Embedding)):\n","            module.to(torch.float16)\n","\n","class CodeGenerator(nn.Module):\n","    DEFAULT_MODEL_NAME = \"Qwen/Qwen1.5-0.5B\"\n","\n","    def __init__(\n","        self,\n","        load_path=None,\n","        model_name_or_path=None,\n","        device=\"cuda\",\n","        max_length=1024,\n","        max_new_tokens=512,\n","        quantization=\"8bit\",\n","        lora=True,\n","        attn_implementation=\"flash_attention_2\",\n","    ):\n","        super().__init__()\n","        model_name_or_path = model_name_or_path or self.DEFAULT_MODEL_NAME\n","\n","        torch.set_float32_matmul_precision('high')\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n","        self.max_length = max_length\n","        self.max_new_tokens = max_new_tokens\n","\n","        if load_path and os.path.exists(os.path.join(load_path, \"tokenizer\")):\n","            tokenizer_path = os.path.join(load_path, \"tokenizer\")\n","        else:\n","            tokenizer_path = model_name_or_path\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n","        self.tokenizer.padding_side = \"left\"\n","\n","        if self.tokenizer.pad_token is None:\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","        # === Model Loading ===\n","        quant_config = get_quant_config(quantization)\n","\n","        if load_path and os.path.exists(load_path):\n","            print(f\"Loading model + LoRA from {load_path}\")\n","            base_model = AutoModelForCausalLM.from_pretrained(\n","                model_name_or_path,\n","                trust_remote_code=True,\n","                device_map=\"auto\",\n","                quantization_config=quant_config,\n","                torch_dtype=torch.float16,\n","                pad_token_id=self.tokenizer.pad_token_id,\n","                eos_token_id=self.tokenizer.eos_token_id,\n","                attn_implementation=attn_implementation\n","            )\n","            base_model = prepare_model_for_kbit_training(base_model)\n","            self.model = PeftModel.from_pretrained(base_model, load_path)\n","        else:\n","            print(f\"Initializing new model from base: {model_name_or_path}\")\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                model_name_or_path,\n","                trust_remote_code=True,\n","                device_map=\"auto\",\n","                quantization_config=quant_config,\n","                torch_dtype=torch.float16,\n","                pad_token_id=self.tokenizer.pad_token_id,\n","                eos_token_id=self.tokenizer.eos_token_id,\n","                attn_implementation=attn_implementation\n","            )\n","            if lora:\n","                print(\"Applying LoRA adaptation...\")\n","                self.model = prepare_model_for_kbit_training(self.model)\n","                lora_config = LoraConfig(\n","                    r=8,\n","                    lora_alpha=16,\n","                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","                    lora_dropout=0.05,\n","                    bias=\"none\",\n","                    task_type=\"CAUSAL_LM\"\n","                )\n","                self.model = get_peft_model(self.model, lora_config)\n","\n","        cast_layernorm_and_embedding_to_half(self.model)\n","        self.model.half()\n","        self.model.config.torch_dtype = torch.float16\n","        self.enable_lora_and_top_blocks_gradients()\n","        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n","        self.model.gradient_checkpointing_disable()\n","\n","        # === Value Head ===\n","        hidden_dim = self.model.config.hidden_size\n","        self.value_head = ValueHead(hidden_dim).to(self.device)\n","\n","        vh_path = os.path.join(load_path or \"\", \"value_head.pt\")\n","        if os.path.exists(vh_path):\n","            self.value_head.load_state_dict(torch.load(vh_path))\n","            print(\"Loaded value head.\")\n","        else:\n","            print(\"No value head to load\")\n","\n","        self.to(self.device)\n","\n","\n","\n","    def save(self, save_path: str):\n","        os.makedirs(save_path, exist_ok=True)\n","\n","        # Save LoRA adapter\n","        if isinstance(self.model, PeftModel):\n","            print(\"Saving LoRA adapter...\")\n","            self.model.save_pretrained(save_path)\n","        else:\n","            print(\"Warning: model is not a PeftModel, skipping LoRA adapter save.\")\n","\n","        # Save value head\n","        if hasattr(self, \"value_head\"):\n","            vh_path = os.path.join(save_path, \"value_head.pt\")\n","            torch.save(self.value_head.state_dict(), vh_path)\n","            print(f\"Saved value head to {vh_path}\")\n","        else:\n","            print(\"Warning: value head not found, skipping.\")\n","\n","        # Save tokenizer\n","        tokenizer_path = os.path.join(save_path, \"tokenizer\")\n","        self.tokenizer.save_pretrained(tokenizer_path)\n","        print(f\"Saved tokenizer to {tokenizer_path}\")\n","\n","    def enable_lora_and_top_blocks_gradients(self):\n","        print(f\"Freezing all parameters except LoRA blocks...\")\n","\n","        # Freeze all parameters\n","        for name, param in self.model.named_parameters():\n","            param.requires_grad = False\n","\n","        # Enable LoRA parameters\n","        for name, param in self.model.named_parameters():\n","            if \"lora\" in name:\n","                param.requires_grad = True\n","\n","        # Print summary\n","        total = sum(p.numel() for p in self.model.parameters())\n","        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","        print(f\"Trainable params: {trainable:,} / {total:,} ({100.0 * trainable / total:.2f}%)\")\n","\n","\n","    def create_code_fixer_prompt(self, bad_code):\n","        return f\"\"\"You are a code style expert specialized in Unreal Engine C++.\n","\n","You must strictly follow the official style guide, including:\n","- Always open braces {{}} on a new line.\n","- Group access specifiers (public, private, protected) properly.\n","- Use UPROPERTY() and UFUNCTION() macros correctly, one variable or function per line.\n","- Use PascalCase naming for variables and functions.\n","- Separate #include statements by engine and project, and add blank lines between groups.\n","- Insert line breaks after semicolons for better readability.\n","\n","Now, fix the following BAD_CODE to fully comply with the style guide.\n","Return only the corrected GOOD_CODE without any explanation.\n","Once you finish writing GOOD_CODE, close the code block by typing ``` and immediately stop generating further text.\n","\n","BAD_CODE:\n","```cpp\n","{bad_code}\n","```\n","GOOD_CODE:\n","\"\"\"\n","\n","\n","    def create_short_prompt_header(self, requirement: str) -> str:\n","        return f\"\"\"You are an Unreal Engine C++ developer.\n","\n","Your task is to implement ONLY the .h file code that corresponds to the given requirement.\n","\n","Do NOT include any explanation, comments, or .cpp file.\n","Do NOT describe the requirement or restate it.\n","Write only the .h code inside a ```cpp code block.\n","Start your C++ code inside a ```cpp code block and close it properly with ```.\n","Always use UCLASS, USTRUCT.\n","Always start with #pragma once\n","\n","---\n","\n","The Requirement below is what you need to implement in Your Answer. **Answer Code Only**\n","\n","Requirement:\n","{requirement}\n","\n","Your Answer:\n","\"\"\"\n","\n","    def create_short_prompt_cpp(self, requirement: str, header_code: str) -> str:\n","        return f\"\"\"You are an Unreal Engine C++ developer.\n","\n","Your task is to implement ONLY the .cpp file code that corresponds to the given requirement and header.\n","\n","Do NOT include any explanation, comments, or .h file.\n","Do NOT describe the requirement or restate it.\n","Write only the .cpp code inside a ```cpp code block.\n","Start your C++ code inside a ```cpp code block and close it properly with ```.\n","\n","---\n","\n","Requirement:\n","{requirement}\n","\n","Corresponding Header File:\n","{header_code}\n","\n","---\n","\n","Now write ONLY the corresponding .cpp file code.\n","Your Answer:\n","\"\"\"\n","\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        self.model.train()\n","        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, use_cache=False)\n","\n","    def sample_code_batch_with_partial_grad_caching(self, prompts: list[str], temperature=0.6, top_p=0.85, repetition_penalty=1.2, max_track_tokens=32):\n","        self.model.train()\n","\n","        inputs = self.tokenizer(\n","            prompts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length\n","        ).to(self.device)\n","\n","        input_ids = inputs[\"input_ids\"]\n","        attention_mask = inputs[\"attention_mask\"]\n","        B = input_ids.size(0)\n","        eos_token_id = self.tokenizer.eos_token_id\n","\n","        generated_token_ids = [[] for _ in range(B)]\n","        all_log_probs = []\n","\n","        for i in range(B):  # 직렬화: 샘플별 처리\n","            cur_input_ids = input_ids[i:i+1]  # [1, T]\n","            cur_attention_mask = attention_mask[i:i+1]\n","            finished = False\n","            code_fence_count = 0\n","            backtick_count = 0\n","            past_key_values = None\n","            cur_log_probs = []\n","\n","            track_flags = torch.zeros(self.max_new_tokens, dtype=torch.bool)\n","\n","            if max_track_tokens == 0:\n","                track_flags[:] = False\n","            else:\n","                # 앞쪽 max_track_tokens 개는 반드시 추적\n","                track_flags[:max_track_tokens] = True\n","\n","                # 그 이후의 토큰 중 일부도 랜덤 추적\n","                additional_track = torch.rand(self.max_new_tokens - max_track_tokens) < 0.025\n","                track_flags[max_track_tokens:] = additional_track\n","\n","\n","            for t in range(self.max_new_tokens):\n","                grad_tracking = track_flags[t].item()\n","\n","                if t == 0:\n","                    inputs_embeds = self.model.get_input_embeddings()(cur_input_ids).to(torch.float16)\n","                else:\n","                    last_token_ids = cur_input_ids[:, -1:]\n","                    inputs_embeds = self.model.get_input_embeddings()(last_token_ids).to(torch.float16)\n","\n","\n","                inputs_embeds = inputs_embeds.to(self.model.dtype)\n","\n","                if grad_tracking:\n","                    inputs_embeds.requires_grad_()\n","                    outputs = self.model(\n","                        inputs_embeds=inputs_embeds,\n","                        attention_mask=cur_attention_mask,\n","                        past_key_values=past_key_values,\n","                        use_cache=True\n","                    )\n","                    past_key_values = outputs.past_key_values\n","                    logits = outputs.logits[:, -1, :] / temperature\n","                    logits = RepetitionPenaltyLogitsProcessor(repetition_penalty)(cur_input_ids, logits)\n","                    logits = TopPLogitsWarper(top_p)(cur_input_ids, logits)\n","                    probs = F.softmax(logits, dim=-1)\n","                    sampled = torch.multinomial(probs, num_samples=1)\n","                    log_prob = torch.log(torch.gather(probs, dim=-1, index=sampled) + 1e-8)\n","                    cur_log_probs.append(log_prob.squeeze(1))\n","                else:\n","                    with torch.no_grad():\n","                        outputs = self.model(\n","                            inputs_embeds=inputs_embeds,\n","                            attention_mask=cur_attention_mask,\n","                            past_key_values=past_key_values,\n","                            use_cache=True\n","                        )\n","                        past_key_values = outputs.past_key_values\n","                        logits = outputs.logits[:, -1, :] / temperature\n","                        logits = RepetitionPenaltyLogitsProcessor(repetition_penalty)(cur_input_ids, logits)\n","                        logits = TopPLogitsWarper(top_p)(cur_input_ids, logits)\n","                        probs = F.softmax(logits, dim=-1)\n","                        sampled = torch.multinomial(probs, num_samples=1)\n","\n","                token_id = sampled[0].item()\n","                token_text = self.tokenizer.decode([token_id], skip_special_tokens=False)\n","\n","                backtick_count += token_text.count(\"`\")\n","\n","                if not finished:\n","                    generated_token_ids[i].append(token_id)\n","\n","                    # 종료 조건 1: EOS\n","                    if token_id == eos_token_id:\n","                        finished = True\n","\n","                    # 종료 조건 2: 코드 fence 감지\n","                    elif token_text.strip() == \"```\":\n","                        code_fence_count += 1\n","                        if code_fence_count >= 2:\n","                            finished = True\n","\n","                    # 종료 조건 3: 백틱 6개 이상\n","                    elif backtick_count >= 6:\n","                        finished = True\n","\n","\n","                cur_input_ids = torch.cat([cur_input_ids, sampled.detach()], dim=1)\n","                cur_attention_mask = torch.cat([cur_attention_mask, torch.ones_like(sampled)], dim=1)\n","\n","                del outputs, logits, probs, sampled, inputs_embeds\n","                if 'log_prob' in locals():\n","                    del log_prob\n","\n","                if finished:\n","                    break\n","\n","            if cur_log_probs:\n","                all_log_probs.append(torch.stack(cur_log_probs))\n","            else:\n","                all_log_probs.append(torch.zeros(1, device=self.device))\n","\n","        final_log_probs = torch.nn.utils.rnn.pad_sequence(all_log_probs, batch_first=True)  # [B, T']\n","        generated_texts = self.tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True)\n","\n","        return {\n","            \"texts\": generated_texts,\n","            \"log_probs\": final_log_probs,\n","        }\n","\n","    def sample_header_with_partial_grad(self, requirements: list[str], max_track_tokens: int = 32) -> dict:\n","        result = {\n","            \"header_texts\": [],\n","            \"header_log_probs\": []\n","        }\n","\n","        header_prompts = [self.create_short_prompt_header(req) for req in requirements]\n","\n","        out = self.sample_code_batch_with_partial_grad_caching(\n","            header_prompts, max_track_tokens=max_track_tokens\n","        )\n","\n","        result[\"header_texts\"] = out[\"texts\"]\n","        result[\"header_log_probs\"] = out[\"log_probs\"]\n","\n","        del out\n","        torch.cuda.empty_cache()\n","\n","        max_len = result[\"header_log_probs\"].shape[1]\n","        result[\"header_log_probs\"] = result[\"header_log_probs\"].to(self.device)\n","\n","        return result\n","\n","    def sample_cpp_with_partial_grad(self, requirements: list[str], header_texts: list[str], max_track_tokens: int = 32) -> dict:\n","        assert len(requirements) == len(header_texts), \"Length mismatch between requirements and headers\"\n","\n","        result = {\n","            \"cpp_texts\": [],\n","            \"cpp_log_probs\": []\n","        }\n","\n","        cpp_prompts = [self.create_short_prompt_cpp(req, header) for req, header in zip(requirements, header_texts)]\n","\n","        cpp_out = self.sample_code_batch_with_partial_grad_caching(\n","            cpp_prompts, max_track_tokens=max_track_tokens\n","        )\n","\n","        result[\"cpp_texts\"] = cpp_out[\"texts\"]\n","        result[\"cpp_log_probs\"] = cpp_out[\"log_probs\"]\n","\n","        del cpp_out\n","        torch.cuda.empty_cache()\n","\n","        result[\"cpp_log_probs\"] = result[\"cpp_log_probs\"].to(self.device)\n","\n","        return result\n","\n","    def compute_value(self, prompts: list[str], responses: list[str], mode: str = \"h\") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        self.value_head.eval()\n","\n","        full_inputs = [p + r for p, r in zip(prompts, responses)]\n","\n","        inputs = self.value_head.tokenizer(\n","            full_inputs,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=512\n","        ).to(self.device)\n","\n","        value_total, value_disc, value_format = self.value_head(\n","            input_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            mode=mode\n","        )\n","\n","        return value_total.squeeze(-1), value_disc.squeeze(-1), value_format.squeeze(-1)\n","\n","    def generate(self, prompts: list[str] | str, return_text: bool = True) -> list[str] | dict[str, list[str]]:\n","        if isinstance(prompts, str):\n","            prompts = [prompts]\n","\n","        with torch.no_grad():\n","            h_result = self.sample_header_with_partial_grad(prompts, 0)\n","            cpp_result = self.sample_cpp_with_partial_grad(prompts, h_result[\"header_texts\"], 0)\n","\n","        if return_text:\n","            return [\n","                h + \"\\n\" + cpp\n","                for h, cpp in zip(h_result[\"header_texts\"], cpp_result[\"cpp_texts\"])\n","            ]\n","        else:\n","            return {\n","                \"header_texts\": h_result[\"header_texts\"],\n","                \"cpp_texts\": cpp_result[\"cpp_texts\"]\n","            }\n","\n","\n","\n","\n","# Main\n","if __name__ == \"__main__\":\n","    generator = CodeGenerator(attn_implementation=\"eager\")\n","    prompt = \"Create a character class with health and mana properties.\"\n","    outputs = generator.generate(prompt)\n","    print(outputs[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfuHKZWEDOBb","executionInfo":{"status":"ok","timestamp":1747835311895,"user_tz":-540,"elapsed":37976,"user":{"displayName":"김락훈","userId":"17655752783041505657"}},"outputId":"bba25097-cce4-4160-b44a-243fc461b4d0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"]},{"output_type":"stream","name":"stdout","text":["Initializing new model from base: Qwen/Qwen1.5-0.5B\n","Applying LoRA adaptation...\n","Freezing all parameters except LoRA blocks...\n","Trainable params: 1,572,864 / 465,560,576 (0.34%)\n","No value head to load\n","```c++\n","struct Character\n","{\n","    int health;\n","    int mana;\n","\n","public:\n","\n","    //Constructor for character\n","    Character(int h)\n","        :health(h),mana(0){}\n","\n","};\n","```\n","\n","```\n","#include <iostream>\n","using namespace std; \n","\n","class Health {\n","private: \n","    int health; \n","};\n","\n","class Mana {\n","private: \n","    int mana; \n","};\n","\n","Character::Health::Health() { \n","    this->health = 150; \n","} \n","\n","Character::Mana::Mana() { \n","    this->mana = 20; \n","}\n","```\n","\n","\n"]}]}]}