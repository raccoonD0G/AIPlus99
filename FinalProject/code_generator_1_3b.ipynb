{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TMRNblivs1yv"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKWj24xGs3Ak"},"outputs":[],"source":["cd /content/drive/MyDrive/Colab Notebooks/Github/AIPlus99/HW6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122056,"status":"ok","timestamp":1747903945510,"user":{"displayName":"김락훈","userId":"17655752783041505657"},"user_tz":-540},"id":"wLVHCRstR1zq","outputId":"efbfa0d8-a800-4666-b1ce-847ff4a0096a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Collecting flash-attn\n","  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n","Building wheels for collected packages: flash-attn\n","  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n","  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n","Successfully built flash-attn\n","Installing collected packages: flash-attn\n","Successfully installed flash-attn-2.7.4.post1\n"]}],"source":["!pip install bitsandbytes\n","!pip install flash-attn --no-build-isolation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["98c3a667293a41bab159989ce3f4cedb","65328f3be8ca4c7b927f0b151a3b5f1c","93325178d9f347c4b441585de5c6ba4a","780a8e2ffb9e4a0b85cc18b2b31f7840","8d2057c7eef64653b070ed9f0bbf9b06","e1e2f2aeddc34d81bcbc0e65618e7844","17bfea2b11a84452a64ad829613db4da","632f7fd9cbbb45eab069da9eaf6a1109","f99e5726bc894f7d88948088fdbc7ca2","48becc65fb934744b4d6ba59a132dd85","f8c76d8ee36b413eae20563e1bb72b80","07ce9a8423d845188796461968164c80","21b762b95ce44bd795241c41388a3263","0cf08e15752d4ab3b51889554a99af44","ee3153b7c4bc467098a6322df6428a51","086e0f4126594a378aa964292c17637a","1ded9b2fe40b4facb22f2d2cd01d090b","cfef6591943b4ed6b674a29029a15c77","a2b25d7f32034f93937af9e9f4ae3421","b35afcb1c90b4e13b4a71eba19fad978","09cc09a572c34fa9be1fd3f1073bd180","b4770c9a68e349288a03e11840069f8b","60d111b0f75748f98661c4fd131c9709","9c5617dfcc064efb81a4cf4078bca738","a6843da9171b403ca5458bb5d42ee59e","1c304f58e4ad4de0baf0cd74235561c7","30e30b6e80c541849086f00cefd25405","d124aa30f7714ac687f015614e138c6f","7bc366e90f4b4c058bf08ab95b49677d","814884856cef41dbb04fa3e2b97389c6","8ee1c236a6bd403da4f547e9c02c8be7","55d90857b2e74e9d877f7e33397a2e28","79e215580eee45bfbcfe8fb0a37e461f","932eb06a5661403eb4472eed4c86294d","322f55b43ce84dfca7820738a1f86692","91326fb90136409f9ff9b4dd6d31a88b","237896cbba624a29a85f075641986441","75cc0192084c48e4ae5bc28c1877d648","eab528dab1434b19b806c6e9d0cddf82","78fd14c7fec44ca78d085bc52970cd41","5286060133834fd09744d33e968db5f4","52c608ee5a1c4c0fbe667718eab7b637","5c4f6c5f1c5b4d9986c5fa618dff9a9d","7ba6a2f7abb7482f8ed1305df371a802","2a1391d58d9846e6994716c9c1052b27","4ca674167df24493a795d16ec490cbf2","3cd6892f0d17465a8134bce2704deafe","ae074d9160b54413865ffc06dcd364e3","4de2c8de2d184532aef7b8ad57b3da86","b17cc2aabe0741c494da52e09df8571c","eee5d957953243d4b1e56230bb72aab9","e4ed4a75b3504182b4665a6ca2fb2c27","d6e8e49c5f0b4e5ba7f8efc481ff7710","5e3660087fdc44c6b3a79dd70ac1dce7","c5f7fc5798854bd591467c2b84639bcc","ca4fb5e815044f0dab25d1018ca15d64","4b5d945e762d4c798d9c5cc686b591b9","b0e313e76e6a44cf92ec8eab5453376b","8ec8efe1d51a4883ac98ab16272ca171","acc6e934abf846149c0366e12258ff24","242936f54e594aea87b4b21276ba6b47","71bd9cfdf9e349d291e5c216c56f27a8","a7b375d74be340f59b8a39bd111fbbe4","743e180794dc43d687293b8b7c5ae7c0","6f800f9c230f4808b7550c9b76ad2a66","a8615467530d4cd9abd47dd1ffa5516f","c3dbca39e4c54ac691f5b80ced566fc9","92a7b188496d4977b6b11694820a5a4a","2e37a2f62c1b4de9a6e45356213fc2e1","216d8aad97204836a6adc4f8a9c46037","adc5d1a26ec64753aeaef4a7af4542e1","c1911965b829408f98e797f813b7aea2","e3341cdf5a274542a6df64306ddcd384","9a03e81a4a684908a8a621062d8ef6c9","d16379176cd440e096d6a2924816a377","6907bb5344e2418b95ab861305ee3121","36dae64af65a443a9804302c43cb17e3","aaea4295661f4e2392554a219c1722f7","b6ae380a36e748169e57b31e129764ec","9d1176667a9d49dfa2de9c851ea3bce1","3540775c3b3b42619e9e2a13c2ca15b6","ff9fa8ea99254327b9b8dfda8a8053bc","bf79c335cb3d4ffca7f7465f6aa5f494","38b97bbeb48b4772a37b7792efdda7a5","b3e8d01141b241de85bfc0eee656a957","bbaf703b7e114d1ea9168fd600b5de33","9fc38ac52cf742238fbb7bc74c61fef3","6f923547a2df4ebc8d853e4dec7c5f3c","31778f8a30be46c091a5c528c870dc7f","fc836c082af146deafe424800593c157","d571156a1af541118096b0488db83b00","0178ae5db6f6490191309724133282a8","6f5621d2375c4f87ad4b29d2db78fda9","e83bd860e1e44feebb8c7d1e8e91c68b","f63ce150f0b74812add9f1ae563bbfcc","ea9e3ab0180740da91f43a251e7fb88c","1acf5b87c84d4e83a0772189b9ca35ab","8dc17ce2d47740ad915365f319a558c9","dcb4f1e084294178afbaea9b3aafa50d","90908b738c844d1b812462fb0045dc9b","76e67ad46aef4033a993d9443dbf6155","139726c49cf742eba72b2f1e6ddfdf9e","084ebfa530f4416891a7b583cf8c1e3f","918155d1ca5940d98f221a1b1b1b344a","6073fb775e66441f9eebb9d817dd2b6c","c178e2cbe89a4de8b471656b7f19a7d5","496027915d364afca6bf2b18e925139b","32f897861d7f4c57951db67736c4b4a2","60d741b4d3db4f2ca36ab9a9dd14addc","2cef881d1aa04a3d866f33604087acaf"]},"executionInfo":{"elapsed":130177,"status":"ok","timestamp":1747904075700,"user":{"displayName":"김락훈","userId":"17655752783041505657"},"user_tz":-540},"id":"zfuHKZWEDOBb","outputId":"b87a571c-fa11-44ad-ff89-985c18ca3f80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model + LoRA from ./checkpoint_1_3b/generator\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98c3a667293a41bab159989ce3f4cedb","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07ce9a8423d845188796461968164c80","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"60d111b0f75748f98661c4fd131c9709","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Freezing all parameters except LoRA blocks...\n","Trainable params: 1,572,864 / 465,560,576 (0.34%)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"932eb06a5661403eb4472eed4c86294d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a1391d58d9846e6994716c9c1052b27","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca4fb5e815044f0dab25d1018ca15d64","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3dbca39e4c54ac691f5b80ced566fc9","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aaea4295661f4e2392554a219c1722f7","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31778f8a30be46c091a5c528c870dc7f","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90908b738c844d1b812462fb0045dc9b","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Loaded value head.\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["\n","=== Sample 1 Header ===\n","```cpp\n","#pragma once\n","\n","#include \"CoreMinimal.h\"\n","#include \"GameFramework/Character.h\"\n","#include \"HealthManaCharacter.generated.h\"\n","\n","UCLASS()\n","class YOURPROJECT_API AHealthManaCharacter : public ACharacter\n","{\n","\tGENERATED_BODY()\n","\n","public:\n","\t// Sets default values for this character's properties\n","\tAHhealthManaCharacter();\n","\n","protected:\n","\t// Called when the game starts or when spawned\n","\tvirtual void BeginPlay() override;\n","\n","public:\n","\t// Called every frame\n","\tvirtual void Tick(float DeltaTime) override;\n","\n","private:\n","\tUPROPERTY(VisibleAnywhere)\n","\tUStaticMeshComponent* MeshComponent;\n","\tUPROPERTY(VisibleAnywhere)\n","\tUHealthComponent* HealthComponent;\n","\tUPROPERTY(VisibleAnywhere)\n","\tUManaComponent* ManaComponent;\n","};\n","```\n","=== Sample 1 CPP ===\n","```cpp\n","#include \"HealthManaCharacter.h\"\n","#include \"Components/StaticMeshComponent.h\"\n","#include \"HealthManaComponent.generated.h\"\n","\n","// Sets default values for this character's properties\n","AHhealthManaCharacter::AHhealthManaCharacter()\n","{\n","\t// Set this character to call Tick() every frame.  You can turn this off to improve performance if you don't need it.\n","\tPrimaryActorTick.bCanEverTick = true;\n","\n","\tMeshComponent = CreateDefaultSubobject<UStaticMeshComponent>(TEXT(\"MeshComponent\"));\n","\tMeshComponent->SetStaticMeshComponent(this);\n","\tMeshComponent->SetMaterial(0, this, UStaticMeshComponent::MaterialType::Default);\n","\tMeshComponent->SetHealthComponent(HealthComponent);\n","\tMeshComponent->SetManaComponent(ManaComponent);\n","}\n","\n","void AHealthManaCharacter::BeginPlay()\n","{\n","\tSuper::BeginPlay();\n","}\n","\n","void AHealthManaCharacter::Tick(float DeltaTime)\n","{\n","\tSuper::Tick(DeltaTime);\n","}\n","\n","UHealthComponent* AHealthManaCharacter::GetHealthComponent() const\n","{\n","\treturn MeshComponent->GetHealthComponent();\n","}\n","\n","UManaComponent* AHealthManaCharacter::GetManaComponent() const\n","{\n","\treturn MeshComponent->GetManaComponent();\n","}\n","```\n","\n","=== Sample 2 Header ===\n","```cpp\n","#pragma once\n","\n","#include \"CoreMinimal.h\"\n","#include \"GameFramework/Actor.h\"\n","#include \"DeathEater.generated.h\"\n","\n","UCLASS()\n","class YOURPROJECT_API ADeathEater : public AActor\n","{\n","\tGENERATED_BODY()\n","\n","public:\n","\tADeathEater();\n","\n","protected:\n","\tvirtual void BeginPlay() override;\n","\n","public:\n","\tvirtual void Tick(float DeltaTime) override;\n","\n","private:\n","\tUPROPERTY(VisibleAnywhere)\n","\tUStaticMeshComponent* MeshComponent;\n","\tUPROPERTY(VisibleAnywhere)\n","\tUPhysicsComponentPhysicsContact* ContactComponent;\n","\tUPROPERTY(VisibleAnywhere)\n","\tUStaticMesh* Mesh;\n","\tvoid OnDeath();\n","};\n","```\n","=== Sample 2 CPP ===\n","```cpp\n","#include \"DeathEater.h\"\n","#include \"Components/StaticMeshComponent.h\"\n","#include \"Physics/Contact.h\"\n","\n","ADeathEater::ADeathEater()\n","{\n","\tPrimaryComponentTick.bCanEverTick = true;\n","\n","\tMeshComponent = CreateDefaultSubobject<UStaticMeshComponent>(TEXT(\"MeshComponent\"));\n","\tMesh = MeshComponent->CreateStaticMesh();\n","\n","\tContactComponent = CreateDefaultSubobject<UPhysicsContact>(TEXT(\"ContactComponent\"));\n","\tContactComponent->SetContactMesh(Mesh);\n","\tContactComponent->SetContactPoint(0, this);\n","}\n","\n","void ADeathEater::BeginPlay()\n","{\n","\tSuper::BeginPlay();\n","}\n","\n","void ADeathEater::Tick(float DeltaTime)\n","{\n","\tSuper::Tick(DeltaTime);\n","\n","\tif (ContactComponent)\n","\t{\n","\t\tContactComponent->OnContact(this, ContactComponent->GetContactPoint(0), DeltaTime);\n","\t}\n","}\n","\n","void ADeathEater::OnDeath()\n","{\n","\tif (Mesh)\n","\t{\n","\t\tContactComponent->OnDeath(this, Mesh);\n","\t}\n","}\n","```\n"]}],"source":["import os\n","import torch\n","import re\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n","from torch.cuda.amp import autocast\n","from value_head import ValueHead\n","from transformers import LogitsProcessorList, RepetitionPenaltyLogitsProcessor, TopPLogitsWarper\n","import torch.nn.functional as F\n","import gc\n","import re\n","\n","def get_quant_config(quantization: str | None):\n","    if quantization == \"4bit\":\n","        return BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=True,\n","            bnb_4bit_quant_type=\"nf4\"\n","        )\n","    elif quantization == \"8bit\":\n","        return BitsAndBytesConfig(\n","            load_in_8bit=True\n","        )\n","    else:\n","        return None\n","\n","def cast_layernorm_and_embedding_to_half(model):\n","    for name, module in model.named_modules():\n","        if isinstance(module, (torch.nn.LayerNorm, torch.nn.Embedding)):\n","            module.to(torch.float16)\n","\n","class CodeGenerator(nn.Module):\n","    DEFAULT_MODEL_NAME = \"Qwen/Qwen1.5-0.5B\"\n","\n","    def __init__(\n","        self,\n","        load_path=None,\n","        model_name_or_path=None,\n","        device=\"cuda\",\n","        max_length=1024,\n","        max_new_tokens=512,\n","        quantization=\"8bit\",\n","        lora=True,\n","        attn_implementation=\"flash_attention_2\",\n","    ):\n","        super().__init__()\n","        model_name_or_path = model_name_or_path or self.DEFAULT_MODEL_NAME\n","\n","        torch.set_float32_matmul_precision('high')\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n","        self.max_length = max_length\n","        self.max_new_tokens = max_new_tokens\n","\n","        if load_path and os.path.exists(os.path.join(load_path, \"tokenizer\")):\n","            tokenizer_path = os.path.join(load_path, \"tokenizer\")\n","        else:\n","            tokenizer_path = model_name_or_path\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n","        self.tokenizer.padding_side = \"left\"\n","\n","        if self.tokenizer.pad_token is None:\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","        # === Model Loading ===\n","        quant_config = get_quant_config(quantization)\n","\n","        if load_path and os.path.exists(load_path):\n","            print(f\"Loading model + LoRA from {load_path}\")\n","            base_model = AutoModelForCausalLM.from_pretrained(\n","                model_name_or_path,\n","                trust_remote_code=True,\n","                device_map=\"auto\",\n","                quantization_config=quant_config,\n","                torch_dtype=torch.float16,\n","                pad_token_id=self.tokenizer.pad_token_id,\n","                eos_token_id=self.tokenizer.eos_token_id,\n","                attn_implementation=attn_implementation\n","            )\n","            base_model = prepare_model_for_kbit_training(base_model)\n","            self.model = PeftModel.from_pretrained(base_model, load_path)\n","        else:\n","            print(f\"Initializing new model from base: {model_name_or_path}\")\n","            self.model = AutoModelForCausalLM.from_pretrained(\n","                model_name_or_path,\n","                trust_remote_code=True,\n","                device_map=\"auto\",\n","                quantization_config=quant_config,\n","                torch_dtype=torch.float16,\n","                pad_token_id=self.tokenizer.pad_token_id,\n","                eos_token_id=self.tokenizer.eos_token_id,\n","                attn_implementation=attn_implementation\n","            )\n","            if lora:\n","                print(\"Applying LoRA adaptation...\")\n","                self.model = prepare_model_for_kbit_training(self.model)\n","                lora_config = LoraConfig(\n","                    r=8,\n","                    lora_alpha=16,\n","                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","                    lora_dropout=0.05,\n","                    bias=\"none\",\n","                    task_type=\"CAUSAL_LM\"\n","                )\n","                self.model = get_peft_model(self.model, lora_config)\n","\n","        cast_layernorm_and_embedding_to_half(self.model)\n","        self.model.half()\n","        self.model.config.torch_dtype = torch.float16\n","        self.enable_lora_and_top_blocks_gradients()\n","        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n","        self.model.gradient_checkpointing_disable()\n","\n","        # === Value Head ===\n","        hidden_dim = self.model.config.hidden_size\n","        self.value_head = ValueHead(hidden_dim).to(self.device)\n","\n","        vh_path = os.path.join(load_path or \"\", \"value_head.pt\")\n","        if os.path.exists(vh_path):\n","            self.value_head.load_state_dict(torch.load(vh_path))\n","            print(\"Loaded value head.\")\n","        else:\n","            print(\"No value head to load\")\n","\n","        self.to(self.device)\n","\n","\n","\n","    def save(self, save_path: str):\n","        os.makedirs(save_path, exist_ok=True)\n","\n","        # Save LoRA adapter\n","        if isinstance(self.model, PeftModel):\n","            print(\"Saving LoRA adapter...\")\n","            self.model.save_pretrained(save_path)\n","        else:\n","            print(\"Warning: model is not a PeftModel, skipping LoRA adapter save.\")\n","\n","        # Save value head\n","        if hasattr(self, \"value_head\"):\n","            vh_path = os.path.join(save_path, \"value_head.pt\")\n","            torch.save(self.value_head.state_dict(), vh_path)\n","            print(f\"Saved value head to {vh_path}\")\n","        else:\n","            print(\"Warning: value head not found, skipping.\")\n","\n","        # Save tokenizer\n","        tokenizer_path = os.path.join(save_path, \"tokenizer\")\n","        self.tokenizer.save_pretrained(tokenizer_path)\n","        print(f\"Saved tokenizer to {tokenizer_path}\")\n","\n","    def enable_lora_and_top_blocks_gradients(self):\n","        print(f\"Freezing all parameters except LoRA blocks...\")\n","\n","        # Freeze all parameters\n","        for name, param in self.model.named_parameters():\n","            param.requires_grad = False\n","\n","        # Enable LoRA parameters\n","        for name, param in self.model.named_parameters():\n","            if \"lora\" in name:\n","                param.requires_grad = True\n","\n","        # Print summary\n","        total = sum(p.numel() for p in self.model.parameters())\n","        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","        print(f\"Trainable params: {trainable:,} / {total:,} ({100.0 * trainable / total:.2f}%)\")\n","\n","\n","    def create_code_fixer_prompt(self, bad_code):\n","        return f\"\"\"You are a code style expert specialized in Unreal Engine C++.\n","\n","You must strictly follow the official style guide, including:\n","- Always open braces {{}} on a new line.\n","- Group access specifiers (public, private, protected) properly.\n","- Use UPROPERTY() and UFUNCTION() macros correctly, one variable or function per line.\n","- Use PascalCase naming for variables and functions.\n","- Separate #include statements by engine and project, and add blank lines between groups.\n","- Insert line breaks after semicolons for better readability.\n","\n","Now, fix the following BAD_CODE to fully comply with the style guide.\n","Return only the corrected GOOD_CODE without any explanation.\n","Once you finish writing GOOD_CODE, close the code block by typing ``` and immediately stop generating further text.\n","\n","BAD_CODE:\n","```cpp\n","{bad_code}\n","```\n","GOOD_CODE:\n","\"\"\"\n","\n","\n","    def create_short_prompt_header(self, requirement: str) -> str:\n","        return f\"\"\"You are an Unreal Engine C++ developer.\n","\n","Your task is to implement ONLY the .h file code that corresponds to the given requirement.\n","\n","Do NOT include any explanation, comments, or .cpp file.\n","Do NOT describe the requirement or restate it.\n","Write only the .h code inside a ```cpp code block.\n","Start your C++ code inside a ```cpp code block and close it properly with ```.\n","Always use UCLASS, USTRUCT.\n","Always start with #pragma once\n","\n","---\n","\n","The Requirement below is what you need to implement in Your Answer. **Answer Code Only**\n","\n","Requirement:\n","{requirement}\n","\n","Your Answer:\n","\"\"\"\n","\n","    def create_short_prompt_cpp(self, requirement: str, header_code: str) -> str:\n","        return f\"\"\"You are an Unreal Engine C++ developer.\n","\n","Your task is to implement ONLY the .cpp file code that corresponds to the given requirement and header.\n","\n","Do NOT include any explanation, comments, or .h file.\n","Do NOT describe the requirement or restate it.\n","Write only the .cpp code inside a ```cpp code block.\n","Start your C++ code inside a ```cpp code block and close it properly with ```.\n","\n","---\n","\n","Requirement:\n","{requirement}\n","\n","Corresponding Header File:\n","{header_code}\n","\n","---\n","\n","Now write ONLY the corresponding .cpp file code.\n","Your Answer:\n","\"\"\"\n","\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        self.model.train()\n","        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, use_cache=False)\n","\n","    def sample_code_batch_with_partial_grad_caching(self, prompts: list[str], temperature=0.3, top_p=0.85, repetition_penalty=1.0, max_track_tokens=32):\n","        self.model.train()\n","\n","        inputs = self.tokenizer(\n","            prompts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length\n","        ).to(self.device)\n","\n","        input_ids = inputs[\"input_ids\"]\n","        attention_mask = inputs[\"attention_mask\"]\n","        B = input_ids.size(0)\n","        eos_token_id = self.tokenizer.eos_token_id\n","\n","        generated_token_ids = [[] for _ in range(B)]\n","        all_log_probs = []\n","\n","        for i in range(B):  # 직렬화: 샘플별 처리\n","            cur_input_ids = input_ids[i:i+1]  # [1, T]\n","            cur_attention_mask = attention_mask[i:i+1]\n","            finished = False\n","            code_fence_count = 0\n","            backtick_count = 0\n","            past_key_values = None\n","            cur_log_probs = []\n","\n","            track_flags = torch.zeros(self.max_new_tokens, dtype=torch.bool)\n","\n","            if max_track_tokens == 0:\n","                track_flags[:] = False\n","            else:\n","                # 앞쪽 max_track_tokens 개는 반드시 추적\n","                track_flags[:max_track_tokens] = True\n","\n","                # 그 이후의 토큰 중 일부도 랜덤 추적\n","                additional_track = torch.rand(self.max_new_tokens - max_track_tokens) < 0.1\n","                track_flags[max_track_tokens:] = additional_track\n","\n","\n","            for t in range(self.max_new_tokens):\n","                grad_tracking = track_flags[t].item()\n","\n","                if t == 0:\n","                    inputs_embeds = self.model.get_input_embeddings()(cur_input_ids).to(torch.float16)\n","                else:\n","                    last_token_ids = cur_input_ids[:, -1:]\n","                    inputs_embeds = self.model.get_input_embeddings()(last_token_ids).to(torch.float16)\n","\n","\n","                inputs_embeds = inputs_embeds.to(self.model.dtype)\n","\n","                if grad_tracking:\n","                    inputs_embeds.requires_grad_()\n","                    outputs = self.model(\n","                        inputs_embeds=inputs_embeds,\n","                        attention_mask=cur_attention_mask,\n","                        past_key_values=past_key_values,\n","                        use_cache=True\n","                    )\n","                    past_key_values = outputs.past_key_values\n","                    logits = outputs.logits[:, -1, :] / temperature\n","                    logits = RepetitionPenaltyLogitsProcessor(repetition_penalty)(cur_input_ids, logits)\n","                    logits = TopPLogitsWarper(top_p)(cur_input_ids, logits)\n","                    probs = F.softmax(logits, dim=-1)\n","                    sampled = torch.multinomial(probs, num_samples=1)\n","                    log_prob = torch.log(torch.gather(probs, dim=-1, index=sampled) + 1e-8)\n","                    cur_log_probs.append(log_prob.squeeze(1))\n","                else:\n","                    with torch.no_grad():\n","                        outputs = self.model(\n","                            inputs_embeds=inputs_embeds,\n","                            attention_mask=cur_attention_mask,\n","                            past_key_values=past_key_values,\n","                            use_cache=True\n","                        )\n","                        past_key_values = outputs.past_key_values\n","                        logits = outputs.logits[:, -1, :] / temperature\n","                        logits = RepetitionPenaltyLogitsProcessor(repetition_penalty)(cur_input_ids, logits)\n","                        logits = TopPLogitsWarper(top_p)(cur_input_ids, logits)\n","                        probs = F.softmax(logits, dim=-1)\n","                        sampled = torch.multinomial(probs, num_samples=1)\n","\n","                token_id = sampled[0].item()\n","                token_text = self.tokenizer.decode([token_id], skip_special_tokens=False)\n","\n","                backtick_count += token_text.count(\"`\")\n","\n","                if not finished:\n","                    generated_token_ids[i].append(token_id)\n","\n","                    # 종료 조건 1: EOS\n","                    if token_id == eos_token_id:\n","                        finished = True\n","\n","                    # 종료 조건 2: 코드 fence 감지\n","                    elif token_text.strip() == \"```\":\n","                        code_fence_count += 1\n","                        if code_fence_count >= 2:\n","                            finished = True\n","\n","                    # 종료 조건 3: 백틱 6개 이상\n","                    elif backtick_count >= 6:\n","                        finished = True\n","\n","\n","                cur_input_ids = torch.cat([cur_input_ids, sampled.detach()], dim=1)\n","                cur_attention_mask = torch.cat([cur_attention_mask, torch.ones_like(sampled)], dim=1)\n","\n","                del outputs, logits, probs, sampled, inputs_embeds\n","                if 'log_prob' in locals():\n","                    del log_prob\n","\n","                if finished:\n","                    break\n","\n","            if cur_log_probs:\n","                all_log_probs.append(torch.stack(cur_log_probs))\n","            else:\n","                all_log_probs.append(torch.zeros(1, device=self.device))\n","\n","        final_log_probs = torch.nn.utils.rnn.pad_sequence(all_log_probs, batch_first=True)  # [B, T']\n","        generated_texts = self.tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True)\n","\n","        return {\n","            \"texts\": generated_texts,\n","            \"log_probs\": final_log_probs,\n","        }\n","\n","    def sample_header_with_partial_grad(self, requirements: list[str], max_track_tokens: int = 32) -> dict:\n","        result = {\n","            \"header_texts\": [],\n","            \"header_log_probs\": []\n","        }\n","\n","        header_prompts = [self.create_short_prompt_header(req) for req in requirements]\n","\n","        out = self.sample_code_batch_with_partial_grad_caching(\n","            header_prompts, max_track_tokens=max_track_tokens\n","        )\n","\n","        result[\"header_texts\"] = out[\"texts\"]\n","        result[\"header_log_probs\"] = out[\"log_probs\"]\n","\n","        del out\n","        torch.cuda.empty_cache()\n","\n","        max_len = result[\"header_log_probs\"].shape[1]\n","        result[\"header_log_probs\"] = result[\"header_log_probs\"].to(self.device)\n","\n","        return result\n","\n","    def sample_cpp_with_partial_grad(self, requirements: list[str], header_texts: list[str], max_track_tokens: int = 32) -> dict:\n","        assert len(requirements) == len(header_texts), \"Length mismatch between requirements and headers\"\n","\n","        result = {\n","            \"cpp_texts\": [],\n","            \"cpp_log_probs\": []\n","        }\n","\n","        cpp_prompts = [self.create_short_prompt_cpp(req, header) for req, header in zip(requirements, header_texts)]\n","\n","        cpp_out = self.sample_code_batch_with_partial_grad_caching(\n","            cpp_prompts, max_track_tokens=max_track_tokens\n","        )\n","\n","        result[\"cpp_texts\"] = cpp_out[\"texts\"]\n","        result[\"cpp_log_probs\"] = cpp_out[\"log_probs\"]\n","\n","        del cpp_out\n","        torch.cuda.empty_cache()\n","\n","        result[\"cpp_log_probs\"] = result[\"cpp_log_probs\"].to(self.device)\n","\n","        return result\n","\n","    def compute_value(self, prompts: list[str], responses: list[str], mode: str = \"h\") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        self.value_head.eval()\n","\n","        full_inputs = [p + r for p, r in zip(prompts, responses)]\n","\n","        inputs = self.value_head.tokenizer(\n","            full_inputs,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=512\n","        ).to(self.device)\n","\n","        value_total, value_disc, value_format = self.value_head(\n","            input_ids=inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            mode=mode\n","        )\n","\n","        return value_total.squeeze(-1), value_disc.squeeze(-1), value_format.squeeze(-1)\n","\n","    def generate(self, prompts: list[str] | str, return_text: bool = True) -> list[str] | dict[str, list[str]]:\n","        if isinstance(prompts, str):\n","            prompts = [prompts]\n","\n","        def extract_or_wrap_cpp_block(text: str) -> str:\n","            match = re.search(r\"(```cpp\\s*.*?\\s*```)\", text, re.DOTALL)\n","            if match:\n","                return match.group(1).strip()\n","            else:\n","                return text.strip()\n","\n","        # === HEADER ===\n","        header_prompts = [self.create_short_prompt_header(req) for req in prompts]\n","        header_inputs = self.tokenizer(header_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(self.device)\n","\n","        with torch.no_grad():\n","            header_outputs = self.model.generate(\n","                **header_inputs,\n","                max_new_tokens=self.max_new_tokens,\n","                do_sample=True,\n","                temperature=0.3,\n","                top_p=0.85,\n","                repetition_penalty=1.0,\n","                return_dict_in_generate=True\n","            )\n","\n","        gen_start = header_inputs[\"input_ids\"].shape[1]\n","        raw_header_texts = [self.tokenizer.decode(seq[gen_start:], skip_special_tokens=True) for seq in header_outputs.sequences]\n","        header_texts = [extract_or_wrap_cpp_block(text) for text in raw_header_texts]\n","\n","        # === CPP ===\n","        cpp_prompts = [self.create_short_prompt_cpp(req, hdr) for req, hdr in zip(prompts, header_texts)]\n","        cpp_inputs = self.tokenizer(cpp_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(self.device)\n","\n","        with torch.no_grad():\n","            cpp_outputs = self.model.generate(\n","                **cpp_inputs,\n","                max_new_tokens=self.max_new_tokens,\n","                do_sample=True,\n","                temperature=0.3,\n","                top_p=0.85,\n","                repetition_penalty=1.0,\n","                return_dict_in_generate=True\n","            )\n","\n","        gen_start = cpp_inputs[\"input_ids\"].shape[1]\n","        raw_cpp_texts = [self.tokenizer.decode(seq[gen_start:], skip_special_tokens=True) for seq in cpp_outputs.sequences]\n","        cpp_texts = [extract_or_wrap_cpp_block(text) for text in raw_cpp_texts]\n","\n","        # === 결과 정리 ===\n","        if return_text:\n","            return [h + \"\\n\" + c for h, c in zip(header_texts, cpp_texts)]\n","        else:\n","            return {\n","                \"header_texts\": header_texts,\n","                \"cpp_texts\": cpp_texts\n","            }\n","\n","\n","# Main\n","if __name__ == \"__main__\":\n","    generator = CodeGenerator(load_path=\"./checkpoint_1_3b/generator\", attn_implementation=\"eager\")\n","    prompts = [\n","        \"Create a character class with health and mana properties.\",\n","        \"Create an enemy class that attacks the player and can die.\"\n","    ]\n","\n","    result = generator.generate(prompts, return_text=False)\n","\n","    for i, (h, c) in enumerate(zip(result[\"header_texts\"], result[\"cpp_texts\"])):\n","        print(f\"\\n=== Sample {i+1} Header ===\\n{h}\")\n","        print(f\"=== Sample {i+1} CPP ===\\n{c}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNWbLR1L4hL+H5H/bO4nHxa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}