{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgoHffkPH1uRtlnqSR0oI/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Eval Loss :\n","https://api.wandb.ai/links/w3yfrl-none/dyjtn8qt\n","\n","Train Loss :\n","https://api.wandb.ai/links/w3yfrl-none/x0w201br"],"metadata":{"id":"8iYoTqg87h_6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7NvCGur7YaO"},"outputs":[],"source":["import os\n","import sys\n","import math\n","import torch\n","import wandb\n","import logging\n","import datasets\n","import argparse\n","import evaluate\n","import transformers\n","\n","from typing import Optional\n","from itertools import chain\n","from dataclasses import dataclass, field\n","\n","from datasets import load_dataset\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    HfArgumentParser,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","\n","wandb.init(project='Hanghae99')\n","wandb.run.name = 'gpt-finetuning'\n","\n","@dataclass\n","class Arguments:\n","    model_name_or_path: Optional[str] = field(default=None)  # HuggingFace hub에서 pre-trained 모델로 사용할 모델의 이름\n","    torch_dtype: Optional[str] = field(default=None, metadata={'choices': ['auto', 'bfloat16', 'float16', 'float32']})  # 우리 모델의 precision(data type이라고 이해하시면 됩니다)\n","\n","    dataset_name: Optional[str] = field(default=None)  # Fine-tuning으로 사용할 huggingface hub에서의 dataset 이름\n","    dataset_config_name: Optional[str] = field(default=None)  # Fine-tuning으로 사용할 huggingface hub에서의 dataset configuration\n","    block_size: int = field(default=1024)  # Fine-tuning에 사용할 input text의 길이\n","    num_workers: Optional[int] = field(default=None)  # Data를 업로드하거나 전처리할 때 사용할 worker 숫자\n","\n","parser = HfArgumentParser((Arguments, TrainingArguments))\n","args, training_args = parser.parse_args_into_dataclasses()\n","\n","logger = logging.getLogger()\n","\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    handlers=[logging.StreamHandler(sys.stdout)],\n",")\n","\n","if training_args.should_log:\n","    transformers.utils.logging.set_verbosity_info()  # log level을 INFO로 변경\n","\n","log_level = training_args.get_process_log_level()\n","\n","# 우리가 가지고 있는 logger와 HuggingFace의 logger의 log level 설정\n","logger.setLevel(log_level)\n","datasets.utils.logging.set_verbosity(log_level)\n","transformers.utils.logging.set_verbosity(log_level)\n","\n","# 기타 HuggingFace logger option들을 설정\n","transformers.utils.logging.enable_default_handler()\n","transformers.utils.logging.enable_explicit_format()\n","\n","logger.info(f\"Training/evaluation parameters {training_args}\")\n","\n","raw_datasets = load_dataset(\n","    args.dataset_name,\n","    args.dataset_config_name\n",")\n","\n","config = AutoConfig.from_pretrained(args.model_name_or_path)\n","tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n","model = AutoModelForCausalLM.from_pretrained(\n","    args.model_name_or_path,\n","    config=config,\n","    torch_dtype=args.torch_dtype\n",")\n","\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\"\n","\n","embedding_size = model.get_input_embeddings().weight.shape[0]\n","if len(tokenizer) > embedding_size:\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","column_names = list(raw_datasets[\"train\"].features)\n","text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n","\n","def tokenize_function(examples):\n","    output = tokenizer(examples[text_column_name])\n","    return output\n","\n","with training_args.main_process_first(desc=\"dataset map tokenization\"):\n","    tokenized_datasets = raw_datasets.map(\n","        tokenize_function,\n","        batched=True,\n","        num_proc=args.num_workers,\n","        remove_columns=column_names\n","    )\n","\n","max_pos_embeddings = config.max_position_embeddings if hasattr(config, \"max_position_embeddings\") else 1024\n","block_size = args.block_size if tokenizer.model_max_length is None else min(args.block_size, tokenizer.model_max_length)\n","\n","def group_texts(examples):\n","    # 주어진 text들을 모두 concat 해줍니다.\n","    # 예를 들어 examples = {'train': [['Hello!'], ['Yes, that is great!']]}이면 결과물은 {'train': ['Hello! Yes, that is great!']}가 됩니다.\n","    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n","\n","    # 전체 길이를 측정합니다.\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    total_length = (total_length // block_size) * block_size\n","\n","    # block_size로 text를 쪼갭니다.\n","    # 예를 들어 block_size=3일 때 {'train': ['Hello! Yes, that is great!']}는\n","    # {'train': ['Hel', 'lo!', ' Ye', 's, ', 'tha', ...]}가 됩니다.\n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","\n","    # Next token prediction이니 label은 자기 자신으로 설정합니다.\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result\n","\n","with training_args.main_process_first(desc=\"grouping texts together\"):\n","    lm_datasets = tokenized_datasets.map(\n","        group_texts,\n","        batched=True,\n","        num_proc=args.num_workers\n","    )\n","\n","train_dataset = lm_datasets[\"train\"]\n","eval_dataset = lm_datasets[\"validation\"] # 추가\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=default_data_collator\n",")\n","\n","checkpoint = None\n","last_checkpoint = get_last_checkpoint(training_args.output_dir)  # 만약 output_dir에 checkpoint가 남아있으면 이를 사용하고, 없으면 None이 return됩니다.\n","if training_args.resume_from_checkpoint is not None:  # output_dir이 아닌 다른 위치에서의 checkpoint를 resume_from_checkpoint로 지정할 수 있습니다.\n","    checkpoint = training_args.resume_from_checkpoint\n","else:  # 아니면 last_checkpoint로 checkpoint를 지정합니다.\n","    checkpoint = last_checkpoint\n","\n","train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","\n","trainer.save_model()\n","\n","metrics = train_result.metrics\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)\n","trainer.save_state()"]}]}