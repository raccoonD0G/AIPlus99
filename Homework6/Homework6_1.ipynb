{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTgNNsMfO5NqLOnaInmv6m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Evel Loss :\n","https://api.wandb.ai/links/w3yfrl-none/2dnx1iup\n","\n","Train Loss :\n","https://api.wandb.ai/links/w3yfrl-none/ldpd35n6"],"metadata":{"id":"bcUNsfW_AVxG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o20HNGkwAGvK"},"outputs":[],"source":["import os\n","import json\n","import torch\n","import wandb\n","import logging\n","from dataclasses import dataclass, field\n","from typing import Optional\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    HfArgumentParser,\n","    default_data_collator,\n",")\n","\n","\n","@dataclass\n","class Arguments:\n","    model_name_or_path: str = field(default=\"gpt2\")\n","    corpus_path: str = field(default=\"corpus.json\")\n","    output_dir: str = field(default=\"./instruction-tuned-gpt\")\n","    block_size: int = field(default=1024)\n","    per_device_train_batch_size: int = field(default=4)\n","    num_train_epochs: int = field(default=3)\n","    save_steps: int = field(default=10)\n","    eval_steps: int = field(default=10)\n","    logging_steps: int = field(default=10)\n","\n","\n","def format_and_tokenize(example, tokenizer, block_size):\n","    prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n","    tokens = tokenizer(\n","        prompt,\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=block_size\n","    )\n","    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n","    return tokens\n","\n","\n","def main():\n","    parser = HfArgumentParser(Arguments)\n","    args = parser.parse_args_into_dataclasses()[0]\n","\n","    wandb.init(project=\"instruction-tuning\")\n","    wandb.run.name = \"homework6_1\"\n","\n","    # 1. Load corpus\n","    dataset = load_dataset(\"json\", data_files={\"data\": args.corpus_path})[\"data\"]\n","\n","    # 2. Split 80/20\n","    split = dataset.train_test_split(test_size=0.2)\n","    train_data = split[\"train\"]\n","    val_data = split[\"test\"]\n","\n","    # 3. Load tokenizer & model\n","    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n","\n","    # 4. Preprocess both datasets\n","    train_dataset = train_data.map(lambda x: format_and_tokenize(x, tokenizer, args.block_size))\n","    val_dataset = val_data.map(lambda x: format_and_tokenize(x, tokenizer, args.block_size))\n","\n","    # 5. Define training arguments\n","    training_args = TrainingArguments(\n","        output_dir=args.output_dir,\n","        per_device_train_batch_size=args.per_device_train_batch_size,\n","        num_train_epochs=args.num_train_epochs,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=args.eval_steps,\n","        save_strategy=\"steps\",\n","        save_steps=args.save_steps,\n","        logging_strategy=\"steps\",\n","        logging_steps=args.logging_steps,\n","        report_to=\"wandb\",\n","        save_total_limit=2\n","    )\n","\n","    # 6. Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        tokenizer=tokenizer,\n","        data_collator=default_data_collator,\n","    )\n","\n","    # 7. Train\n","    trainer.train()\n","    trainer.save_model()\n","    print(\"Fine-tuning complete.\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}