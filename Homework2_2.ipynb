{"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from tqdm import tqdm\n","from Homework2_1 import TextToImageClassifier\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\")\n","tokenizer.src_lang = \"en_XX\"\n","tokenizer.tgt_lang = \"fr_XX\"\n","\n","\n","csv_path = \"eng_-french/eng_-french.csv\"\n","df = pd.read_csv(csv_path, header=None)\n","df.columns = [\"English\", \"French\"]\n","df.dropna(inplace=True)\n","df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)\n","train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n","\n","def tokenize_target(texts, max_length=64):\n","    tokenized = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n","    return tokenized[\"input_ids\"]\n","\n","train_targets = tokenize_target(train_data[\"French\"].tolist())\n","test_targets = tokenize_target(test_data[\"French\"].tolist())\n","\n","class TranslationDataset(Dataset):\n","    def __init__(self, sources, targets):\n","        self.sources = sources\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.sources)\n","\n","    def __getitem__(self, idx):\n","        return self.sources[idx], self.targets[idx]\n","\n","train_dataset = TranslationDataset(train_data[\"English\"].tolist(), train_targets)\n","test_dataset = TranslationDataset(test_data[\"English\"].tolist(), test_targets)\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=16)\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len).unsqueeze(1)\n","        div = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div)\n","        pe[:, 1::2] = torch.cos(pos * div)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","\n","class TextToTranslationModel(nn.Module):\n","    def __init__(self, text_image_encoder, tgt_vocab_size, d_model=512, num_layers=4, nhead=8, dim_feedforward=2048):\n","        super().__init__()\n","\n","        self.text_image_encoder = text_image_encoder\n","        self.project_fused = nn.Linear(1536, d_model)\n","\n","        self.embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model)\n","\n","        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n","        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n","        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n","\n","\n","    def forward(self, sentence_list, tgt_input, tgt_mask=None):\n","        sentence_pairs = [(s, s) for s in sentence_list]\n","\n","        memory = self.text_image_encoder.extract_features(sentence_pairs)\n","        memory = self.project_fused(memory).unsqueeze(1).transpose(0, 1)\n","\n","        tgt_emb = self.embedding(tgt_input)\n","        tgt_emb = self.pos_encoding(tgt_emb).transpose(0, 1)\n","\n","        out = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n","        return self.fc_out(out.transpose(0, 1))\n","\n","    def generate(self, sentence, max_len=50, sos_token=tokenizer.lang_code_to_id[\"fr_XX\"], eos_token=2):\n","        self.eval()\n","        generated = [sos_token]\n","        for _ in range(max_len):\n","            tgt_input = torch.tensor([generated], dtype=torch.long).to(next(self.parameters()).device)\n","            logits = self.forward([sentence], tgt_input)\n","            next_token = logits[0, -1].argmax(-1).item()\n","            generated.append(next_token)\n","            if next_token == eos_token:\n","                break\n","        return tokenizer.decode(generated, skip_special_tokens=True)\n","\n","\n","text_image_encoder = TextToImageClassifier().to(\"cuda\")\n","model = TextToTranslationModel(text_image_encoder, tgt_vocab_size=tokenizer.vocab_size).to(\"cuda\")\n","optimizer = Adam(model.parameters(), lr=5e-5)\n","loss_fn = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n","\n","for epoch in range(10):\n","    model.train()\n","    total_loss = 0\n","    for src_texts, tgt_ids in tqdm(train_loader):\n","        decoder_input = tgt_ids[:, :-1].to(\"cuda\")\n","        decoder_target = tgt_ids[:, 1:].to(\"cuda\")\n","        outputs = model(src_texts, decoder_input)\n","        loss = loss_fn(outputs.reshape(-1, outputs.size(-1)), decoder_target.reshape(-1))\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n","\n","    print(\"\\n=== Random Translation Samples ===\")\n","    model.eval()\n","\n","    # 랜덤 샘플 10개 추출\n","    samples = test_data.sample(10, random_state=42).reset_index(drop=True)\n","\n","    for i in range(len(samples)):\n","        en = samples.loc[i, \"English\"]\n","        fr = samples.loc[i, \"French\"]\n","        pred = model.generate(en)\n","\n","        print(f\"[{i + 1}]\")\n","        print(f\"EN (input): {en}\")\n","        print(f\"FR (true) : {fr}\")\n","        print(f\"FR (pred) : {pred}\")\n","        print(\"-\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"lUncRlKSHdJT","executionInfo":{"status":"error","timestamp":1744312454323,"user_tz":-540,"elapsed":12443,"user":{"displayName":"김락훈","userId":"17655752783041505657"}},"outputId":"1e677006-f44e-4894-a307-d4cc8e66781f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'Homework2_1'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-81da530bce42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mHomework2_1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextToImageClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ------------------------ Tokenizer ------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Homework2_1'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1Q8Co2FWHxjftQw3hZmk4SjF3lyse4MZR","timestamp":1744291831468}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}